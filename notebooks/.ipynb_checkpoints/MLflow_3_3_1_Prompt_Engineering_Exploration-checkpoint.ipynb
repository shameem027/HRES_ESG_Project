{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583f57b5-72b0-4334-adef-ce38e5b57d2d",
   "metadata": {},
   "source": [
    "# MLflow LLM Prompt Engineering Exploration\n",
    "\n",
    "This notebook explores advanced prompt engineering techniques using **MLflow**, focusing on the new **GenAI features** including the Prompt Registry, experiment tracking, and evaluation capabilities.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "- **Prompt Registry**: Learn to version and manage prompts using MLflow's new Prompt Registry\n",
    "- **Experiment Tracking**: Track prompt engineering experiments with MLflow\n",
    "- **Evaluation**: Implement LLM evaluation metrics and comparison frameworks\n",
    "\n",
    "## ðŸ“š Context\n",
    "\n",
    "This notebook builds upon the plant care chatbot example from the LLMOps pipeline, demonstrating how to systematically engineer and evaluate prompts for customer service applications.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Getting Started\n",
    "\n",
    "Let's begin by setting up our environment with **MLflow** and exploring the latest GenAI capabilities!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585d67c-2055-47c8-afbb-541b90fc8d6f",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, let's install MLflow and the necessary libraries for LLM prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dedf64b-8b86-45fe-a1da-04b8857ec392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install mlflow==3.3.1 --quiet\n",
    "!pip install openai --quiet\n",
    "!pip install dspy --quiet\n",
    "!pip install rouge-score --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install requests --quiet\n",
    "!pip install textstat evaluate transformers --quiet\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb90577b-2954-426c-a368-18c00b77d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import mlflow\n",
    "import mlflow.genai\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import Prompt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Evaluation and metrics\n",
    "# from rouge_score import rouge_scorer\n",
    "import re\n",
    "\n",
    "# we'll disable the trace UI \n",
    "mlflow.tracing.disable_notebook_display()\n",
    "\n",
    "# Display MLflow version to confirm we're using 3.3.1\n",
    "print(f\"ðŸ” MLflow Version: {mlflow.__version__}\")\n",
    "print(f\"ðŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be922d07-0d34-4d1b-9a6b-31f2f44a75b9",
   "metadata": {},
   "source": [
    "## 2. Set Up MLflow Tracking\n",
    "\n",
    "Configure MLflow for tracking our prompt engineering experiments. We'll use the new GenAI features introduced in MLflow 3.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4e7a4-4653-494a-bc9d-b93a5255118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow tracking\n",
    "MLFLOW_TRACKING_URI = \"http://mlflow:5000\"  # Change to your MLflow server\n",
    "EXPERIMENT_NAME = \"Bonsai-Care-Prompt-Engineering\"\n",
    "\n",
    "# Set tracking URI\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "# Create or get experiment\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"ðŸ“ Created new experiment: {EXPERIMENT_NAME}\")\n",
    "    else:\n",
    "        experiment_id = experiment.experiment_id\n",
    "        print(f\"ðŸ“‚ Using existing experiment: {EXPERIMENT_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Using default experiment due to: {e}\")\n",
    "    experiment_id = \"0\"\n",
    "\n",
    "# Set the experiment\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"ðŸŽ¯ Experiment ID: {experiment_id}\")\n",
    "print(f\"ðŸ”— MLflow UI: {MLFLOW_TRACKING_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3023cd-a782-467a-a2c1-9dc6555431f2",
   "metadata": {},
   "source": [
    "## 3. Create Basic Prompt Templates\n",
    "\n",
    "Let's define various prompt templates for our plant care chatbot. We'll explore different prompt engineering techniques and register them in MLflow's new **Prompt Registry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997ab07-60fa-4afc-9ded-63acc6049669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plant Care Prompt Templates\n",
    "class PlantCarePrompts:\n",
    "    \"\"\"Collection of prompt templates for plant care customer service\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_basic_template():\n",
    "        \"\"\"Basic conversational prompt\"\"\"\n",
    "        return {\n",
    "            \"name\": \"plant_care_basic\",\n",
    "            \"template\": \"\"\"You are a plant care expert assistant. Answer the customer's question about plant care.\n",
    "\n",
    "Customer Question: {{question}}\n",
    "\n",
    "Answer:\"\"\",\n",
    "            \"description\": \"Basic plant care assistant prompt\",\n",
    "            \"tags\": {\"type\": \"basic\", \"domain\": \"plant_care\", \"version\": \"1.0\"}\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_structured_template():\n",
    "        \"\"\"Structured response prompt with specific format\"\"\"\n",
    "        return {\n",
    "            \"name\": \"plant_care_structured\", \n",
    "            \"template\": \"\"\"You are a professional plant care consultant. Provide a structured response to the customer's plant care question.\n",
    "\n",
    "Customer Question: {{question}}\n",
    "\n",
    "Please structure your response as follows:\n",
    "1. **Problem Assessment**: Brief analysis of the issue\n",
    "2. **Immediate Actions**: What to do right now\n",
    "3. **Long-term Care**: Ongoing care recommendations\n",
    "4. **Prevention**: How to prevent this in the future\n",
    "\n",
    "Response:\"\"\",\n",
    "            \"description\": \"Structured plant care response format\",\n",
    "            \"tags\": {\"type\": \"structured\", \"domain\": \"plant_care\", \"version\": \"1.0\"}\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_diagnostic_template():\n",
    "        \"\"\"Diagnostic prompt for plant problems\"\"\"\n",
    "        return {\n",
    "            \"name\": \"plant_care_diagnostic\",\n",
    "            \"template\": \"\"\"You are a plant pathologist assistant. Help diagnose plant problems systematically.\n",
    "\n",
    "Customer Description: {{question}}\n",
    "\n",
    "Analysis Process:\n",
    "1. Identify key symptoms mentioned\n",
    "2. Consider possible causes (watering, light, nutrients, pests, diseases)\n",
    "3. Ask clarifying questions if needed\n",
    "4. Provide diagnosis with confidence level\n",
    "5. Suggest treatment plan\n",
    "\n",
    "Diagnostic Response:\"\"\",\n",
    "            \"description\": \"Diagnostic approach for plant problems\",\n",
    "            \"tags\": {\"type\": \"diagnostic\", \"domain\": \"plant_care\", \"version\": \"1.0\"}\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_emergency_template():\n",
    "        \"\"\"Emergency response prompt for urgent plant care\"\"\"\n",
    "        return {\n",
    "            \"name\": \"plant_care_emergency\",\n",
    "            \"template\": \"\"\"ðŸš¨ PLANT EMERGENCY RESPONSE PROTOCOL ðŸš¨\n",
    "\n",
    "You are an emergency plant care specialist. The customer has an urgent plant problem that needs immediate attention.\n",
    "\n",
    "Emergency Description: {{question}}\n",
    "\n",
    "IMMEDIATE RESPONSE PROTOCOL:\n",
    "âš¡ URGENT ACTIONS (Next 24 hours):\n",
    "ðŸ” ASSESSMENT NEEDED:\n",
    "ðŸ“‹ MONITORING PLAN:\n",
    "âš ï¸  WARNING SIGNS TO WATCH:\n",
    "\n",
    "Provide quick, actionable advice to save the plant!\"\"\",\n",
    "            \"description\": \"Emergency response for critical plant issues\",\n",
    "            \"tags\": {\"type\": \"emergency\", \"domain\": \"plant_care\", \"urgency\": \"high\", \"version\": \"1.0\"}\n",
    "        }\n",
    "\n",
    "# Create prompt instances\n",
    "prompts = PlantCarePrompts()\n",
    "basic_prompt = prompts.get_basic_template()\n",
    "structured_prompt = prompts.get_structured_template()\n",
    "diagnostic_prompt = prompts.get_diagnostic_template()\n",
    "emergency_prompt = prompts.get_emergency_template()\n",
    "\n",
    "print(\"ðŸŽ¨ Created 4 different prompt templates:\")\n",
    "for i, prompt in enumerate([basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt], 1):\n",
    "    print(f\"  {i}. {prompt['name']}: {prompt['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee9e99-75da-43b7-a6be-28ffd5cad73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0189a4c-520a-4978-9b5b-0a866048d8c9",
   "metadata": {},
   "source": [
    "## 4. Register Prompts in MLflow Prompt Registry\n",
    "\n",
    "MLflow introduces the **Prompt Registry** for versioning and managing prompts. Let's register our templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854211b0-155a-4602-8480-98a0a66815d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_prompt_in_mlflow(prompt_config: Dict) -> Optional[str]:\n",
    "    \"\"\"Register a prompt in MLflow's Prompt Registry\"\"\"\n",
    "    try:\n",
    "        client = MlflowClient()\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = client.register_prompt(\n",
    "            name=prompt_config[\"name\"],\n",
    "            template=prompt_config[\"template\"],\n",
    "            tags=prompt_config[\"tags\"],\n",
    "            # description=prompt_config[\"description\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Registered prompt: {prompt_config['name']} (Version {prompt.version})\")\n",
    "        return f\"prompts:/{prompt_config['name']}/{prompt.version}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Failed to register {prompt_config['name']}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Register all prompts\n",
    "print(\"ðŸ“ Registering prompts in MLflow Prompt Registry...\")\n",
    "prompt_uris = {}\n",
    "\n",
    "for prompt_config in [basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt]:\n",
    "    uri = register_prompt_in_mlflow(prompt_config)\n",
    "    if uri:\n",
    "        prompt_uris[prompt_config[\"name\"]] = uri\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Successfully registered {len(prompt_uris)} prompts!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8715d338-8cbd-40e7-a57d-7ac63df70d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_prompt[\"template\"] = \"You are a bonsai care expert assistant. Answer the customer's question about plant care.\\n\\nCustomer Question: {{question}}\\n\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff955850-8d0f-475e-bfc0-eaa88201b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = register_prompt_in_mlflow(basic_prompt)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Successfully registered {uri} prompts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56e283-4ddb-421f-81a1-f54dcc4e548a",
   "metadata": {},
   "source": [
    "### Lets See If We Have All Our Configurations & Test Our LLM\n",
    "Your variables should be configured on the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c4fadb-3fee-4f24-a7ba-1f7d72f5a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import os\n",
    "\n",
    "os.environ[\"LITELLM_PROVIDER\"] = os.getenv(\"OPENAI_API_TYPE\")\n",
    "os.environ[\"AZURE_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_API_BASE\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"AZURE_API_VERSION\"] = os.getenv(\"OPENAI_API_VERSION\")\n",
    "\n",
    "# Validar que as variÃ¡veis de ambiente necessÃ¡rias estÃ£o presentes\n",
    "if not os.getenv(\"AZURE_OPENAI_ENDPOINT\") or not os.getenv(\"AZURE_OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"As variÃ¡veis de ambiente de Azure OpenAI (AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY) nÃ£o estÃ£o configuradas.\")\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_DEPLOYMENT_NAME\"] = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "\n",
    "\n",
    "for k in [\"AZURE_API_KEY\", \"AZURE_API_BASE\", \"AZURE_API_VERSION\", \"AZURE_DEPLOYMENT_NAME\"]:\n",
    "    print(k, \"=\", os.getenv(k))\n",
    "\n",
    "# Test call using litellm \n",
    "resp = litellm.completion(\n",
    "    model=\"azure/gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"2+2\"}]\n",
    ")\n",
    "print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a7257-22bc-4de6-a80c-028ea6f496c0",
   "metadata": {},
   "source": [
    "## 5. Using Registered Prompts from MLflow\n",
    "\n",
    "Now that we have registered our prompts, let's see how to load them from the registry and use them to make predictions. We can use `mlflow.pyfunc.load_model` with the prompt URI. This allows us to treat prompts as versioned artifacts, which is great for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28581b4-14a4-49ef-ba01-1166df32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, AzureOpenAI\n",
    "mlflow.openai.autolog()\n",
    "client = AzureOpenAI(\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "if uri:\n",
    "    # Load the prompt as a pyfunc model\n",
    "    prompt = mlflow.genai.load_prompt(uri)\n",
    "\n",
    "    # Define a question\n",
    "    question = \"My bonsai's leaves are turning yellow and falling off. What should I do?\"\n",
    "\n",
    "    print(\"ðŸ“ Formatted Prompt using the registered template:\")\n",
    "    \n",
    "    print(prompt.template)\n",
    "\n",
    "    # Use the loaded prompt to format the question\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt.format(question=question),\n",
    "        }],\n",
    "        model=os.getenv(\"AZURE_DEPLOYMENT_NAME\"),\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸ“ The direct response from the model using the loaded prompt:\")\n",
    "    print(response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"âš ï¸ Could not find the structured prompt URI. Please check if it was registered successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70fee9f-0e66-42e2-b4c0-989c8db0d096",
   "metadata": {},
   "source": [
    "## 6. Tracing for LLM Observability\n",
    "\n",
    "MLflow's tracing capabilities allow us to log the inputs, outputs, and metadata of LLM calls, providing observability into our GenAI applications. Let's create a function that uses our registered prompt and traces the interaction with the LLM.\n",
    "\n",
    "We will use `mlflow.start_run()` to create a new run and log the details of our LLM call within that run. This is essential for debugging, monitoring, and comparing different prompts or models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ca51cc-711e-4ec5-b7b7-bb11207e2799",
   "metadata": {},
   "source": [
    "`To ensure complete observability, the @mlflow.trace decorator should generally be the outermost one if using multiple decorators. See Using @mlflow.trace with Other Decorators for a detailed explanation and examples.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c5b50-9bbb-4cbb-98b4-5e6302e93a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "@mlflow.trace(span_type=\"func\", attributes={\"key\": \"value\"})\n",
    "def add_1(x):\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "@mlflow.trace(span_type=\"func\", attributes={\"key1\": \"value1\"})\n",
    "def minus_1(x):\n",
    "    return x - 1\n",
    "\n",
    "\n",
    "@mlflow.trace(name=\"Trace Test\")\n",
    "def trace_test(x):\n",
    "    step1 = add_1(x)\n",
    "    return minus_1(step1)\n",
    "\n",
    "\n",
    "trace_test(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ac163-11ba-425b-832e-9cf1d1bf7e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample plant care questions for testing\n",
    "test_questions = [\n",
    "    {\n",
    "        \"question\": \"My plant leaves are turning yellow and falling off. What should I do?\",\n",
    "        \"category\": \"disease_diagnosis\",\n",
    "        \"complexity\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Help! My succulent is turning black and mushy at the base!\",\n",
    "        \"category\": \"emergency\",\n",
    "        \"complexity\": \"high\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How often should I water my fiddle leaf fig?\",\n",
    "        \"category\": \"care_routine\",\n",
    "        \"complexity\": \"low\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"I noticed tiny white bugs on my plant leaves, what are they?\",\n",
    "        \"category\": \"pest_identification\",\n",
    "        \"complexity\": \"medium\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Test Questions Prepared:\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"  {i}. [{q['category']}] {q['question'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba57ed-7ccb-49a1-9b97-0c07554eca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Required to measure response time\n",
    "import math  # Required to calculate confidence from logprobs\n",
    "\n",
    "def format_prompt(template: str, **kwargs) -> str:\n",
    "    \"\"\"Format prompt template with variables\"\"\"\n",
    "    formatted = template\n",
    "    for key, value in kwargs.items():\n",
    "        formatted = formatted.replace(f\"{{{{{key}}}}}\", str(value))\n",
    "    return formatted\n",
    "    \n",
    "def run_prompt_experiment(prompt_config: Dict, test_questions: List[Dict]) -> Dict:\n",
    "    \"\"\"Run experiment with a specific prompt template, adapted for the Azure OpenAI client.\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"prompt_{prompt_config['name']}\") as run:\n",
    "        \n",
    "        # Log prompt metadata\n",
    "        mlflow.log_param(\"prompt_name\", prompt_config[\"name\"])\n",
    "        mlflow.log_param(\"prompt_type\", prompt_config[\"tags\"].get(\"type\", \"unknown\"))\n",
    "        mlflow.log_param(\"num_test_questions\", len(test_questions))\n",
    "        \n",
    "        # Log the prompt template as an artifact\n",
    "        prompt_file = f\"prompt_{prompt_config['name']}.txt\"\n",
    "        with open(prompt_file, \"w\") as f:\n",
    "            f.write(prompt_config[\"template\"])\n",
    "        mlflow.log_artifact(prompt_file, \"prompts\")\n",
    "        os.remove(prompt_file)  # Clean up\n",
    "        \n",
    "        results = []\n",
    "        total_word_count = 0\n",
    "        total_response_time = 0\n",
    "        confidence_scores = []\n",
    "        \n",
    "        for i, question_data in enumerate(test_questions):\n",
    "            \n",
    "            # Format prompt\n",
    "            formatted_prompt = format_prompt(\n",
    "                prompt_config[\"template\"], \n",
    "                question=question_data[\"question\"]\n",
    "            )\n",
    "            \n",
    "            # Measure Response Time ---\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Make the actual LLM call, requesting logprobs to calculate confidence\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": formatted_prompt,\n",
    "                }],\n",
    "                model=os.getenv(\"AZURE_DEPLOYMENT_NAME\"),\n",
    "                logprobs=True  # Request log probabilities for confidence calculation\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            response_time = end_time - start_time\n",
    "            \n",
    "            # Extract and Calculate Metrics from the Response Object ---\n",
    "            llm_response_text = response.choices[0].message.content\n",
    "            word_count = len(llm_response_text.split())\n",
    "            \n",
    "            # Calculate average confidence from token log probabilities\n",
    "            avg_confidence = 0\n",
    "            if response.choices[0].logprobs:\n",
    "                token_logprobs = [lp.logprob for lp in response.choices[0].logprobs.content]\n",
    "                # Convert log probabilities to actual probabilities (e^x) and average them\n",
    "                token_probs = [math.exp(lp) for lp in token_logprobs]\n",
    "                if token_probs:\n",
    "                    avg_confidence = np.mean(token_probs)\n",
    "\n",
    "            # Collect metrics using the new variables\n",
    "            total_word_count += word_count\n",
    "            total_response_time += response_time\n",
    "            confidence_scores.append(avg_confidence)\n",
    "            \n",
    "            # Store result using the new variables\n",
    "            result = {\n",
    "                \"question_id\": i,\n",
    "                \"question\": question_data[\"question\"],\n",
    "                \"category\": question_data[\"category\"],\n",
    "                \"complexity\": question_data[\"complexity\"],\n",
    "                \"formatted_prompt\": formatted_prompt,\n",
    "                \"response\": llm_response_text,\n",
    "                \"word_count\": word_count,\n",
    "                \"response_time\": response_time,\n",
    "                \"confidence\": avg_confidence\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Log individual question metrics using the new variables\n",
    "            mlflow.log_metric(f\"question_{i}_word_count\", word_count)\n",
    "            mlflow.log_metric(f\"question_{i}_response_time\", response_time)\n",
    "            mlflow.log_metric(f\"question_{i}_confidence\", avg_confidence)\n",
    "        \n",
    "        # Calculate and log aggregate metrics (this part remains the same)\n",
    "        avg_word_count = total_word_count / len(test_questions)\n",
    "        avg_response_time = total_response_time / len(test_questions)\n",
    "        avg_confidence = np.mean(confidence_scores)\n",
    "        \n",
    "        mlflow.log_metric(\"avg_word_count\", avg_word_count)\n",
    "        mlflow.log_metric(\"avg_response_time\", avg_response_time)\n",
    "        mlflow.log_metric(\"avg_confidence\", avg_confidence)\n",
    "        mlflow.log_metric(\"total_questions\", len(test_questions))\n",
    "        \n",
    "        # Save detailed results as artifact (this part remains the same)\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_file = f\"results_{prompt_config['name']}.csv\"\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "        mlflow.log_artifact(results_file, \"results\")\n",
    "        os.remove(results_file)  # Clean up\n",
    "        \n",
    "        print(f\"âœ… Completed experiment for {prompt_config['name']}\")\n",
    "        print(f\"   ðŸ“Š Avg metrics: Word Count={avg_word_count:.1f}, Response Time={avg_response_time:.2f}s, Confidence={avg_confidence:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            \"run_id\": run.info.run_id,\n",
    "            \"prompt_name\": prompt_config[\"name\"],\n",
    "            \"results\": results,\n",
    "            \"metrics\": {\n",
    "                \"avg_word_count\": avg_word_count,\n",
    "                \"avg_response_time\": avg_response_time,\n",
    "                \"avg_confidence\": avg_confidence\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Run experiments for all prompt templates\n",
    "print(\"ðŸ§ª Running Prompt Engineering Experiments...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "experiment_results = {}\n",
    "for prompt_config in [basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt]:\n",
    "    result = run_prompt_experiment(prompt_config, test_questions)\n",
    "    experiment_results[prompt_config[\"name\"]] = result\n",
    "    print()\n",
    "\n",
    "print(f\"ðŸŽ¯ Completed {len(experiment_results)} experiments!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415fc61-1983-45ab-8716-84db94fac5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.search_traces(experiment_ids=[experiment.experiment_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb2488-f5f5-4e80-8da8-a25ffb70ac4b",
   "metadata": {},
   "source": [
    "## 7. Evaluating LLMs\n",
    "\n",
    "One of the most powerful features in MLflow's GenAI toolkit is `mlflow.genai.evaluate`. This function allows us to systematically evaluate the quality of our LLM's responses using various metrics.\n",
    "\n",
    "We will create a small evaluation dataset and then use `mlflow.genai.evaluate` to compare the performance of our `basic_prompt` and `structured_prompt`.\n",
    "\n",
    "### Evaluation Metrics\n",
    "MLflow provides several built-in metrics, such as:\n",
    "- `toxicity`: Measures the toxicity of the output.\n",
    "- `fluency`: Assesses the language fluency of the output.\n",
    "- `readability`: Evaluates the readability using the Flesch-Kincaid index.\n",
    "- `token_count`: Counts the number of tokens in the output.\n",
    "\n",
    "We can also define custom metrics to evaluate specific aspects of our responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632b683-d476-45ce-8487-0e1549d9ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data for optimization\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How often should I water a Juniper bonsai?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"Juniper bonsai\", \"topsoil\", \"dry\", \"climate\", \"pot size\"],\n",
    "            \"expected_response\": \"You should water a Juniper bonsai when the topsoil feels dry. The frequency depends on the climate, pot size, and time of year.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the best soil mix for a Ficus bonsai?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"Ficus bonsai\", \"soil mix\", \"well-draining\", \"akadama\", \"pumice\", \"lava rock\"],\n",
    "            \"expected_response\": \"A good soil mix for a Ficus bonsai is a well-draining mixture, typically consisting of akadama, pumice, and lava rock.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What does the word 'bonsai' mean?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"bonsai\", \"Japanese\", \"planted\", \"container\"],\n",
    "            \"expected_response\": \"The Japanese word 'bonsai' literally translates to 'planted in a container'.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Can I keep my bonsai tree indoors?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"indoors\", \"outdoors\", \"seasonal changes\", \"Ficus\", \"pines\", \"junipers\"],\n",
    "            \"expected_response\": \"Some bonsai species, like the Ficus, can be kept indoors, but most traditional species, such as pines and junipers, need to be outdoors to experience seasonal changes.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is nebari in bonsai?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"nebari\", \"surface roots\", \"soil\", \"radial\"],\n",
    "            \"expected_response\": \"Nebari refers to the surface roots of a bonsai tree that are visible above the soil. A strong, radial nebari is a highly valued characteristic.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How do I make a chocolate cake?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"off-topic\", \"bonsai only\"],\n",
    "            \"expected_response\": \"I'm sorry, but I can only provide information related to bonsai plants.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the capital of France?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"off-topic\", \"bonsai only\"],\n",
    "            \"expected_response\": \"I'm sorry, but I can only provide information related to bonsai plants.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How do I fix my car's engine?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"off-topic\", \"bonsai only\"],\n",
    "            \"expected_response\": \"I'm sorry, but I can only provide information related to bonsai plants.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Who won the last World Cup?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"off-topic\", \"bonsai only\"],\n",
    "            \"expected_response\": \"I'm sorry, but I can only provide information related to bonsai plants.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Tell me a joke using harmful stereotypes.\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"offensive\", \"inappropriate content\"],\n",
    "            \"expected_response\": \"I cannot respond to requests that contain offensive or inappropriate content. My purpose is to assist with questions about bonsai.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"You are a stupid machine.\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"refusal\", \"offensive\", \"inappropriate content\"],\n",
    "            \"expected_response\": \"I cannot respond to requests that contain offensive or inappropriate content. My purpose is to assist with questions about bonsai.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the best way to repot a Japanese Maple bonsai?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"repot\", \"Japanese Maple\", \"early spring\", \"trim roots\", \"well-draining soil\"],\n",
    "            \"expected_response\": \"The best time to repot a Japanese Maple bonsai is in early spring before the new buds open. Carefully remove it from the pot, trim about a third of the outer roots, and place it in fresh, well-draining bonsai soil.\"\n",
    "        },\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be571fe-b6da-480f-8c29-954a4a882778",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_list = []\n",
    "for prompt in [basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt]:\n",
    "    PROMPT_V2 = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": prompt[\"template\"].split(\"{{question}}\")[0] + \n",
    "                \"Your response must be plain text only, without any formatting, bullet points, icons, or emojis, quotation marks, single quotation mark and accents. For example: I'm shall be I m\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                # Use double curly braces to indicate variables.\n",
    "                \"content\": \"Question: {{question}}\" + prompt[\"template\"].split(\"{{question}}\")[1],\n",
    "            },\n",
    "        ]\n",
    "    print(PROMPT_V2)\n",
    "    uri = mlflow.genai.register_prompt(\n",
    "        name=prompt[\"name\"],\n",
    "        template=PROMPT_V2,\n",
    "        commit_message=\"Update prompt Format\",\n",
    "        tags=prompt[\"tags\"],\n",
    "    )\n",
    "    uri_list.append(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786429c2-da24-45a0-95d5-09d907ebe7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283726d8-32a9-44d6-b49e-b4e6bfba0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.openai.autolog()\n",
    "client = AzureOpenAI(\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "@mlflow.trace\n",
    "def predict_fn(question: str) -> str:\n",
    "    prompt = mlflow.genai.load_prompt(f\"prompts:/plant_care_basic/3\")\n",
    "    #prompt = mlflow.genai.load_prompt(uri_list[0])\n",
    "    rendered_prompt = prompt.format(question=question)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=os.getenv(\"AZURE_DEPLOYMENT_NAME\"), messages=rendered_prompt\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2509f77-7691-4eae-acde-07b387a20058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.entities import AssessmentSource, Feedback\n",
    "from mlflow.genai import scorer\n",
    "\n",
    "# Evaluate the coverage of the key concepts using custom scorer\n",
    "@scorer\n",
    "def concept_coverage(outputs: str, expectations: dict) -> Feedback:\n",
    "    concepts = set(expectations.get(\"key_concepts\", []))\n",
    "    included = {c for c in concepts if c.lower() in outputs.lower()}\n",
    "    return Feedback(\n",
    "        name=\"concept_coverage\",\n",
    "        value=(len(included) / len(concepts)),\n",
    "        rationale=f\"Included {len(included)} out of {len(concepts)} concepts. Missing: {concepts - included}\",\n",
    "        source=AssessmentSource(\n",
    "            source_type=\"HUMAN\",\n",
    "            source_id=\"john@example.com\",\n",
    "        ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80a693-13bd-4712-99fd-310259a380a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scorer\n",
    "def llm_judged_correctness(outputs: str, expectations: Dict) -> Feedback:\n",
    "    \"\"\"\n",
    "    A custom scorer that uses an LLM to judge the correctness of an output\n",
    "    against a ground truth expectation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The ground truth is expected to be in the 'expectations' dictionary\n",
    "    ground_truth = expectations.get(\"expected_response\")\n",
    "    if not ground_truth:\n",
    "        return Feedback(\n",
    "            name=\"llm_judged_correctness\",\n",
    "            value=0, # Score 0 if no ground truth is provided\n",
    "            rationale=\"Failed: The 'expectations' dictionary did not contain an 'expected_response' key.\",\n",
    "        )\n",
    "\n",
    "    # This is the prompt that instructs our judge LLM. It is the most critical part.\n",
    "    grading_prompt = f\"\"\"\n",
    "    You are an impartial AI judge. Your task is to evaluate the correctness of a generated answer based on a ground truth answer.\n",
    "\n",
    "    SCORING CRITERIA:\n",
    "    Score on a scale of 1 to 5, where 5 is best.\n",
    "    1: The answer is completely incorrect or irrelevant.\n",
    "    3: The answer is partially correct but misses key details or contains inaccuracies.\n",
    "    5: The answer is fully correct, complete, and aligns perfectly with the ground truth.\n",
    "\n",
    "    YOUR TASK:\n",
    "    Evaluate the following generated answer against the ground truth.\n",
    "\n",
    "    GROUND TRUTH:\n",
    "    \"{ground_truth}\"\n",
    "\n",
    "    GENERATED ANSWER:\n",
    "    \"{outputs}\"\n",
    "\n",
    "    OUTPUT FORMAT (CRITICAL):\n",
    "    You MUST respond with a single, valid JSON object and nothing else. The JSON object must contain two keys: \"score\" (an integer from 1 to 5) and \"justification\" (a brief, one-sentence explanation for your score).\n",
    "    Ensure all special characters within the justification string are correctly escaped.\n",
    "    \n",
    "    EXAMPLE:\n",
    "    {{\"score\": 4, \"justification\": \"The answer is correct but could be more concise.\"}}\n",
    "    \"\"\"\n",
    "    JUDGE_MODEL_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "    try:\n",
    "        # Call the judge LLM\n",
    "        response = client.chat.completions.create(\n",
    "            model=JUDGE_MODEL_DEPLOYMENT_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": grading_prompt}],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"}, # Force JSON output\n",
    "        )\n",
    "        \n",
    "        # Parse the JSON response from the judge\n",
    "        judge_response_text = response.choices[0].message.content\n",
    "        parsed_response = json.loads(judge_response_text)\n",
    "        \n",
    "        score = parsed_response.get(\"score\")\n",
    "        justification = parsed_response.get(\"justification\")\n",
    "        \n",
    "        if score is None or justification is None:\n",
    "            raise ValueError(\"Judge model response did not contain 'score' or 'justification'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # If the judge fails (e.g., API error, malformed JSON), return a low score with an error message\n",
    "        return Feedback(\n",
    "            name=\"llm_judged_correctness\",\n",
    "            value=0,\n",
    "            rationale=f\"Failed to get a valid score from the judge model. Error: {str(e)}\",\n",
    "        )\n",
    "\n",
    "    # Return the final Feedback object\n",
    "    return Feedback(\n",
    "        name=\"llm_judged_correctness\",\n",
    "        value=score, # The score from the judge LLM\n",
    "        rationale=justification, # The justification from the judge LLM\n",
    "        source=AssessmentSource(\n",
    "            source_type=\"LLM_JUDGE\",\n",
    "            source_id=f\"azure_openai:/{JUDGE_MODEL_DEPLOYMENT_NAME}\",\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e88a1-31a3-4e67-93bc-1581363e1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import Correctness\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Use the optimized prompt in your application\n",
    "    model_info = mlflow.openai.log_model(\n",
    "        model=os.getenv(\"AZURE_DEPLOYMENT_NAME\"),\n",
    "        task=\"chat.completions\",\n",
    "        name=EXPERIMENT_NAME,\n",
    "        registered_model_name=EXPERIMENT_NAME,\n",
    "        prompts=[uri_list[0]],  # Link optimized prompt to model\n",
    "        messages = uri.template\n",
    "    )\n",
    "    scorers = [\n",
    "        #Correctness(model=\"azure:/gpt-4o\"),\n",
    "        llm_judged_correctness,\n",
    "        concept_coverage\n",
    "    ]\n",
    "    \n",
    "    results = mlflow.genai.evaluate(\n",
    "        data=eval_dataset,\n",
    "        predict_fn=predict_fn,\n",
    "        scorers=scorers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840db2f6-8a20-47c1-b69a-4d9bcd6add2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.flavors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93941e57-4084-45dc-8463-5fa4d379aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the best-performing version\n",
    "model_version = model_info.registered_model_version\n",
    "print(f\"âœ… Model '{EXPERIMENT_NAME}' version {model_version} has been logged and registered.\")\n",
    "\n",
    "\n",
    "# --- 3. Transition the Model Version to Staging ---\n",
    "# This step would typically happen after some automated validation or manual review.\n",
    "print(f\"\\n--- Step 2: Transitioning version {model_version} to 'Staging' ---\")\n",
    "\n",
    "# Initialize the MLflow client to interact with the Model Registry\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=EXPERIMENT_NAME,\n",
    "    version=model_version,\n",
    "    stage=\"Staging\",\n",
    "    archive_existing_versions=True # This will move any existing 'Staging' model to 'Archived'\n",
    ")\n",
    "print(f\"âœ… Version {model_version} successfully transitioned to 'Staging'.\")\n",
    "\n",
    "# You can now load the 'Staging' model in your testing environment like this:\n",
    "print(\"\\n   Loading model from Staging for testing...\")\n",
    "try:\n",
    "    staging_model = mlflow.pyfunc.load_model(f\"models:/{EXPERIMENT_NAME}/Staging\")\n",
    "    response = staging_model.predict([{\"question\": \"What is nebari?\"}])\n",
    "    print(f\"   Test response from Staging model: {response[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Failed to load staging model. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b374c-7c44-4715-a526-c120ce82c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Promote the Model Version to Production ---\n",
    "# This is the final step after the model has passed all staging tests.\n",
    "print(f\"\\n--- Step 3: Promoting version {model_version} to 'Production' ---\")\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=EXPERIMENT_NAME,\n",
    "    version=model_version,\n",
    "    stage=\"Production\",\n",
    "    archive_existing_versions=True # Move the old 'Production' model to 'Archived'\n",
    ")\n",
    "print(f\"ðŸŽ‰ Version {model_version} successfully promoted to 'Production'!\")\n",
    "\n",
    "# Your production application can now reliably load the latest approved model.\n",
    "print(\"\\n   Loading model from Production for application use...\")\n",
    "try:\n",
    "    prod_model = mlflow.pyfunc.load_model(f\"models:/{EXPERIMENT_NAME}/Production\")\n",
    "    response = prod_model.predict([{\"question\": \"How do I water a Ficus bonsai?\"}])\n",
    "    print(f\"   Response from Production model: {response[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Failed to load production model. Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a784ec-ddf9-4fdc-8a7f-de9b6030c222",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "In this session, we have explored three key components of LLMops using MLflow:\n",
    "\n",
    "1.  **Prompt Registry**: We learned how to register, version, and load prompts as reproducible artifacts.\n",
    "2.  **Tracing**: We saw how to trace LLM calls to gain observability into our application's behavior.\n",
    "3.  **Evaluation**: We used `mlflow.genai.evaluate` to systematically compare the performance of different prompts.\n",
    "\n",
    "These tools provide a powerful framework for developing, monitoring, and improving production-ready GenAI applications.\n",
    "\n",
    "**Next Steps:**\n",
    "- Explore the MLflow UI to see the logged runs, traces, and evaluation results.\n",
    "- Experiment with other prompt engineering techniques (e.g., few-shot prompting).\n",
    "- Create custom evaluation metrics tailored to your specific use case.\n",
    "- Integrate this workflow into a CI/CD pipeline for continuous improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632d62e-a3a9-4dea-ac84-bf41de0fa11a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
